\documentclass{article}

\usepackage{graphicx}
\usepackage{xeCJK}
\usepackage{bm}
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{cite}
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green,CJKbookmarks=true]{hyperref}
\usepackage{indentfirst}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage[margin=3.5cm]{geometry}
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage{amssymb}
% \linespread{1.6}
\geometry{left=3.2cm,right=3.2cm,top=3.2cm,bottom=3.2cm}
\usepackage{multirow}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{lipsum}
\setenumerate[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\setitemize[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\setdescription{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
%定理
\makeatletter
\thm@headfont{\sc}
\makeatother
\newtheorem{theorem}{Theorem}
%%%%%%%%%%%%% C++ Code
\usepackage{color}
\usepackage{xcolor}
\definecolor{keywordcolor}{rgb}{0.8,0.1,0.5}
\usepackage{listings}
\lstset{breaklines}%这条命令可以让LaTeX自动将长的代码行换行排版
\lstset{extendedchars=false}%这一条命令可以解决代码跨页时，章节标题，页眉等汉字不显示的问题
\lstset{ %用于设置语言为C++
    keywordstyle=\color{keywordcolor} \bfseries, 
    %identifierstyle=,
    basicstyle=\ttfamily, 
    commentstyle=\color{blue} \textit,
    stringstyle=\ttfamily, 
    showstringspaces=false,
    frame=shadowbox, %边框
    %captionpos=b
}


%\setCJKmainfont[BoldFont = 黑体]{宋体}
\setlength{\parindent}{2em}
%如果不要缩进 用\noindent
\title{Deep Learning for Natural Language Processing \\ (cs224d) \\ Lecture Notes}

\author{\large Yihong Gu\footnote{gyh15@mails.tsinghua.edu.cn}}

\date{}

\begin{document}

\maketitle

\tableofcontents

\newpage

\section{NLP and DL: introduction}

\subsection{Natural Language Processing}

\subsubsection*{Hierarchical Stages}

首先，需要了解关于Natural Language Processing的几个层次：

\begin{itemize}
	\item \textbf{Phonetic Analysis}/\textbf{OCR}阶段：主要解决的是自然语言的输入端的问题，其中\textbf{Phonetic}为语音、\textbf{OCR}为文本。
	\item \textbf{Morphological Analysis}: 主要解决词的合成的问题，比如词的前缀和后缀的意义(e.g. un-)。
	\item \textbf{Syntactic Analysis}: 主要解决句法合成的问题，研究词如何通过语法结构进行组合。
	\item \textbf{Semantic Interpretation}: 主要解决句子含义(meaning)的问题。
	\item \textbf{Discourse Processing}: 主要解决整篇文章的理解的问题。
\end{itemize}

\subsubsection*{Difficulty}

传统的NLP的困难之处主要在以下两个方面，第一个方面是知识(knowledge)的表示，以下面句子为例

\begin{center}
\begin{quote}
Jane hit June and then \textbf{she} [fell/ran]
\end{quote}
\end{center}

显然，fell和ran分别让she有了不同的指代(Jane/June)，如果要让机器能够准确作出正确的指代关系的话，仅仅考虑单词的意思和语法结构是不够的，机器需要理解hit和fell/ran之间的逻辑关系，这需要进一步的先验知识。

第二个方面是模糊性(ambiguity)，即多重含义。

\subsection{Deep Learning}

\subsubsection*{Representation Learning}

我们考虑一个传统的(预测类)机器学习过程，我们把它分成两部分

$$\text{Machine Learning} = \text{Feature Engineering}+\text{Learning Algorithm}$$

在这样的过程中，我们拿到数据(data)，先要人工设计数据的特征表示(representation)，然后再使用特定的机器学习算法。通常而言，特征表示在整个过程的地位更高并且需要花费更多的精力。

而Deep Learning是什么？我们可以把它看成一种特征学习(representation learning)，它能够从大量的数据中自动地学习特征。在具体实践中，Deep Learning更加灵活，适用性也更强，一个适用于某个问题的DL模型可以直接运用于另外一个问题上。

\subsubsection*{Visualization}

值得注意的是，可视化(\textbf{visualization})在DL中非常重要，我们通常会(使用PCA等方法)把我们从DL中学到的feature投影到二维空间作可视化，我们可以从中发现一些非常有趣的性质：对于NLP问题来说，一个非常广泛的性质在于意思相近的单词/短语/句子在这样的欧几里得空间中也靠的更近。

\subsubsection*{DL+NLP}

我们考虑DL在NLP上的应用，其核心在于把单词/短语/句子映射成$d$维空间中的向量，进而进行进一步的分析。

\section{Word Vector}

Word Vector的意义在于把单词映射成$d$维空间中的向量。关于word vector(以及之后的NLP)，我们有一些相关的术语：

\begin{itemize}
	\item \textbf{corpus}: 指用来作训练的文本全集。
	\item \textbf{token}: 文本中的某个单一的元素，可以是一个单词，也可以是一个标点，也可以是开始符／结束符。
\end{itemize}

传统的NLP中，关于单词有三种表示方法：

\begin{itemize}
	\item one-hot: 单词的词向量$\in \mathbb{R}^{\lvert V\rvert}$，其中$\lvert V\rvert$为单词总数。
	\item taxonomy: 建立词与词之间的从属关系(is-a)，比如flower从属于plant(flower \textbf{is-a} plant)，相关的工作有WordNet。
	\item synonym set: 建立同义词/近义词集合。
\end{itemize}

\subsection{Statistical Based Method: SVD}

\subsubsection*{Model}

首先，我们通过文本构造\textbf{Window based Co-occurence Matrix}，考虑构造这样一个矩阵$X \in \mathbb{M}_{\lvert V\rvert \times \lvert V\rvert}$，其\textbf{window size}为$W$，那么，如果单词$w_j$在单词$w_i$的大小为$W$的window中出现，$X_{i,j}$就累加$1$，下面是一个例子

\begin{quote}
\begin{itemize}
	\item[1.] I enjoy flying.
	\item[2.] I like NLP.
	\item[3.] I like deep learning.
\end{itemize}
\end{quote}

其对应的co-occurence matrix $X$为：

\begin{equation}X=\label{cooccurence_matrix}\bordermatrix{
& I & like & enjoy & deep & learning & NLP & flying & .\cr
       I & 0 & 2 & 1 & 0 & 0 & 0 & 0 & 0\cr
    like & 2 & 0 & 0 & 1 & 0 & 1 & 0 & 0\cr
   enjoy & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0\cr
    deep & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0\cr
learning & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1\cr
     NLP & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1\cr
  flying & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1\cr
       . & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0
}\end{equation}

注意到这里的矩阵$X$为对称矩阵，我们对其作SVD，令$X=\mathcal{U}\mathrm{S}\mathcal{V}$，其中$\mathcal{U},\mathcal{V} \in \mathbb{M}_{\lvert V\rvert\times \lvert V\rvert}$并且是正定矩阵，$\mathrm{S} \in \mathbb{M}_{\lvert V\rvert\times\lvert V\rvert}$并且是对角矩阵，记$\sigma_i=S_{i,i}$，取$\mathcal{U}$的前$K$列$\mathcal{U}^{[,1:K]}$作为最后的$K$维word vector，这时候第$i$行的向量$\mathrm{u}_i=\mathcal{U}^{[,1:K]}_i$即为单词$w_i$的word vector。

\subsubsection*{Drawbacks and Extensions}

这样的模型主要有以下几个问题：

\begin{itemize}
	\item 由矩阵本身引起的问题：矩阵太大并且非常稀疏、加入新单词后拓展非常耗时间。
	\item 关于co-occurence的问题：需要解决部分词出现太频繁所导致的问题(比如the etc.)。
\end{itemize}

我们考虑这两类的问题的解决方案，首先是co-occurence的问题，我们可以用以下的方案来解决：

\begin{itemize}
	\item [1.] 令$X^*_{i,j}=min(X_{i,j}, t)$，其中$t \sim 100$
	\item [2.] 使用pearson correlation，同时让负值取0从而代替原来的简单的计数。
	\item [3.] 使用ramp window：给window的不同位置加权，一个比较自然的想法是距离更近的位置的权值更高。
\end{itemize}

我们无法很好的解决第一个问题，这就引入了下面的\ref{word2vec}的内容。

\subsection{Iteration Based Methods: word2vec}
\label{word2vec}

我们在本节的模型中都会运用到以下记号：

\begin{itemize}
	\item $w_i$为单词集(vocabulary)中的第$i$个单词
	\item $n$为单词空间的维数
	\item $\mathcal{V}\in\mathbb{R}^{n\times\lvert V\rvert}$为input word matrix，其中$\mathrm{v}_i$是$\mathcal{V}$的第$i$列，表示单词$w_i$的input vector
	\item $\mathcal{U}\in\mathbb{R}^{n\times\lvert V\rvert}$output word matrix，其中$\mathrm{u}_i$是$\mathcal{U}$的第$i$列，表示单词$w_i$output vector
	\item window size为$m$。
\end{itemize}

\subsubsection*{CBOW Model}

在CBOW(Continuous Bag of Word)模型中，我们考虑一个问题：使用周围的单词(context)预测中间的单词(center word)，我们考虑使用以下的预测模型结构：

假设我们需要预测的单词是$w^{(c)}$，其周围的单词为$w^{(c-m)},\cdots,w^{(c-1)},w^{(c+1)},\cdots,w^{(c+m)}$，为了方便表示，我们使用这些单词的one-hot表示形式$\mathrm{x}^{(c-m)},\cdots,\mathrm{x}^{(c-1)},\mathrm{x}^{(c+1)},\cdots,\mathrm{x}^{(c+m)}$，则相应的单词$w^{(c+i)}$所对应的input vector为

\begin{eqnarray}
\mathrm{v}^{(c+i)}=\mathcal{V} \mathrm{x}^{(c+i)}
\end{eqnarray}

我们对这$2m$个向量取平均，得到一个$n$维向量$\hat{\mathrm{v}}$

\begin{eqnarray}
\hat{\mathrm{v}}=\frac{1}{2m}(\mathrm{v}^{(c-m)}+\cdots+\mathrm{v}^{(c-1)}+\mathrm{v}^{(c+1)}+\cdots+\mathrm{v}^{(c+m)})
\end{eqnarray}

接下来我们得到了一个得分向量(score vector)$\mathrm{z}$，这里$\mathrm{z}\in\mathbb{R}^{\lvert V\rvert}$其第$i$个元素表示$\hat{\mathrm{v}}$和第$i$个单词的输出向量的相似程度，这里的相似程度用点乘的大小来度量)：

\begin{eqnarray}
\mathrm{z}=\mathcal{U}\hat{\mathrm{v}}
\end{eqnarray}

最后我们把得分向量作一个softmax变换得到中心的单词的概率分布

\begin{eqnarray}
\hat{\mathrm{y}} = \mathrm{softmax}(\mathrm{z})
\end{eqnarray}

我们定义损失函数(统计/决策论意义上的)$H$为：

\begin{eqnarray}
H(\mathrm{y},\hat{\mathrm{y}})=-\sum_{y=1}^{\lvert V\rvert}{y_i\log(\hat{y}_i)}
\end{eqnarray}

实际上，如果让$\mathrm{y}$为center word的one-hot向量，其中$y_k=1$，那么，$H(\mathrm{y},\hat{\mathrm{y}})$可以简化为

\begin{eqnarray}
H(\mathrm{y},\hat{\mathrm{y}})=-y_k\log(\hat{y}_k)
\end{eqnarray}

我们写出最后的object function的化简形式

\begin{eqnarray}
J &=& -\log\mathbb{P}(\mathrm{u_c}\lvert w^{(c-m)},\cdots,w^{(c-1)},w^{(c+1)},\cdots,w^{(c+m)}) \\
  &=& -\log\frac{\exp(\mathrm{u}_c^T\hat{\mathrm{v}})}{\sum_{j=1}^{\lvert V\rvert}{\exp(\mathrm{u}_j^T\hat{\mathrm{v}})}} \\
  &=& -\mathrm{u}_c^T\hat{\mathrm{v}} + \log\sum_{j=1}^{\lvert V\rvert}{\exp(\mathrm{u}_j^T\hat{\mathrm{v}})}
\end{eqnarray}

其中$\mathrm{u}_c$为center word的output vector。

结合上面的表达式，我们需要对这两个vector有一个直观的认识，实际上，我们的优化目标是让每个center word周围的单词的input vector的平均值尽量接近其output vector。

\subsubsection*{Skip-Gram Model}

Skip-Gram Model的方向正好和CBOW相反，在Skip-Gram中，我们给定中间的center word，需要预测周围的context，为了简化描述，我们直接给出概率形式的表达：式[\ref{skipprobsingle}]给出了给定中间单词$w_c$，单词$w_o$在其window中出现的概率；式[\ref{skiplossprob}]给出了损失函数的形式，这里我们使用了近一步的独立性假设$\mathbb{P}(w_{c-m},\cdots,w_{c-1},c_{c+1},\cdots,w_{c+m}\lvert w_c)=\prod_{j=-m,j\neq 0}^{m} \mathbb{P}(w_{c+j}\lvert w_c)$(虽然这非常不符合实际)；我们把[\ref{skipprobsingle}]代入[\ref{skiplossprob}]并且进一步化简得到式[\ref{skiploss}]

\begin{eqnarray} \label{skipprobsingle}
\mathbb{P}(w_o\lvert w_c) &=& \frac{\exp(\mathrm{u}_o^T\mathrm{v}_c)}{\sum_{k=1}^{\lvert V\rvert}{\exp(\mathrm{u}_k^T\mathrm{v}_c)}} \\
\label{skiplossprob}
J &=& -\log\prod_{j=-m,j\neq 0}^{m} \mathbb{P}(w_{c+j}\lvert w_c) \\
\label{skiploss}
J &=& -\sum_{j=-m,j\neq 0}^{m}{\mathrm{u}_{c+j}^T\mathrm{v}_c}+2m\log\sum_{k=1}^{\lvert V\rvert}{\exp(\mathrm{u}_k^T\mathrm{v}_c)}
\end{eqnarray}

我们给出梯度的表达式：

\begin{eqnarray}
\frac{\partial J}{\partial \mathrm{v}_c}&=&-\sum_{j=-m,j\neq 0}^{m}{\mathrm{u}_{c+j}^T}+2m\sum_{x=1}^{\lvert V\rvert}{\frac{\exp(\mathrm{u}_x^T\mathrm{v}_c)}{\sum_{k=1}^{\lvert V\rvert}{\exp(\mathrm{u}_k^T\mathrm{v}_c)}}\mathrm{u}_x^T}\\
\frac{\partial J}{\partial \mathrm{u}_x}&=&m \frac{\exp(\mathrm{u}_x^T\mathrm{v}_c)}{\sum_{k=1}^{\lvert V\rvert}{\exp(\mathrm{u}_k^T\mathrm{v}_c)}}\mathrm{v}_c^T - y_x\mathrm{v}_c^T
\end{eqnarray}

但是，最后我们的形式仅考虑一对$\mathbb{P}(o\lvert c)$，最后的object function的结果如式[\ref{skipof}]，梯度的表达式如式[\ref{skipgrad1}]和[\ref{skipgrad2}]

\begin{eqnarray}
\label{skipof}
J &=& -\mathrm{u}_{c+j}^T\mathrm{v}_c + \log\sum_{k=1}^{\lvert V\rvert}{\exp(\mathrm{u}_k^T\mathrm{v}_c)} \\
\label{skipgrad1}
\frac{\partial J}{\partial \mathrm{v}_c}&=&-\mathrm{u}_{c+j}^T+\sum_{x=1}^{\lvert V\rvert}{\frac{\exp(\mathrm{u}_x^T\mathrm{v}_c)}{\sum_{k=1}^{\lvert V\rvert}{\exp(\mathrm{u}_k^T\mathrm{v}_c)}}\mathrm{u}_x^T} \\
\label{skipgrad2}
\frac{\partial J}{\partial \mathrm{u}_x}&=&\frac{\exp(\mathrm{u}_x^T\mathrm{v}_c)}{\sum_{k=1}^{\lvert V\rvert}{\exp(\mathrm{u}_k^T\mathrm{v}_c)}}\mathrm{v}_c^T - y_x\mathrm{v}_c^T
\end{eqnarray}

\subsubsection*{Negative Sampling}

考虑到以上的模型在实际计算中运算量非常大，所以我们考虑另外一种近似方法，这种近似方法的阐述也分成两个部分：第一个部分是object function，第二个部分是gradient。

首先，我们考虑Skip-Gram Model的Negative Sampling的近似优化方法：

我们使用$(x,c)$来分别表示$(\text{context},\text{center})$，同时记$D=\{(x,c)\lvert w_x=w^{(c+j)}, -m\le j\le m, j\neq 0\}$，那么，我们可以得到另外一种$\mathcal{P}(w_o\lvert w_c)$的表达式[\ref{nsskipprobsingle}]

\begin{eqnarray}
\label{nsskipprobsingle}
\mathbb{P}\big((o,c)\in D\big) = \sigma(\mathrm{u}_o^T\mathrm{v}_c) = \frac{1}{1+\exp(-\mathrm{u}_o^T\mathrm{v}_c)}
\end{eqnarray}

同时使用独立性条件，我们写出object function的表达式[\ref{ofskip}]，将[\ref{nsskipprobsingle}]的结果代入，得到最终的表达式[\ref{ofskipf}]

\begin{eqnarray}
\label{ofskip}
J &=& -\log\Big\{\prod_{(o,c) \in D}{\mathbb{P}\big((o,c) \in D\big)}\prod_{(x,c)}{\mathbb{P}\big((w,c) \in \bar{D}\big)}\Big\} \\
\label{ofskipf}
J &=& -\sum_{(o,c) \in D}{\log(\sigma(\mathrm{u}_o^T\mathrm{v}_c))} - \sum_{(w,c) \in \bar{D}}{\log(\sigma(-\mathrm{u}_w^T\mathrm{v}_c))}
\end{eqnarray}

我们把$\bar{D}$称为nagative corpus，在具体求解的时候，我们在$\bar{D}$中随机抽$K$个组成(一对的)object function [\ref{ofnsskip}]，其梯度为 [\ref{ofnsskipgrad1}], [\ref{ofnsskipgrad2}] 和 [\ref{ofnsskipgrad3}]，其中每个单词被抽到的概率服从分布 [\ref{ofnsdis}]：

\begin{eqnarray}
\label{ofnsskip}
J &=& -\log(\sigma(\mathrm{u}_{c+j}^T\mathrm{v}_c)) - \sum_{k=1}^K{\log(\sigma(-\bar{\mathrm{u}}_k^T\mathrm{v}_c))}\\
\label{ofnsskipgrad1}
\frac{\partial J}{\partial \mathrm{v}_c} &=& -\big(1-\sigma(\mathrm{u}_{c+j}^T\mathrm{v}_c)\big)\mathrm{u}_{c+j}^T + \sum_{k=1}^K{\big(1-\sigma(-\bar{\mathrm{u}}_k^T\mathrm{v}_c)\big)\bar{\mathrm{u}}_k^T} \\
\label{ofnsskipgrad2}
\frac{\partial J}{\partial \mathrm{u}_{c+j}} &=& -\big(1-\sigma(\mathrm{u}_{c+j}^T\mathrm{v}_c)\big)\mathrm{v}_{c}^T \\
\label{ofnsskipgrad3}
\frac{\partial J}{\partial \bar{\mathrm{u}}_k} &=& \big(1-\sigma(-\bar{\mathrm{u}}_k^T\mathrm{v}_c)\big)\mathrm{v}_{c}^T \\
\label{ofnsdis}
\mathbb{P}(w) &\propto& \big(\text{frequency of word $w$}\big)^{3/4}
\end{eqnarray}

同样，我们考虑CBOW Model的Negative Sampling的近似优化方法：

沿用之前的记号，类似的，我们可以得出object function：

\begin{eqnarray}
J &=& -\log(\sigma(\mathrm{u}_c^T\hat{\mathrm{v}})) - \sum_{w \neq c}{\log(\sigma(-\mathrm{u}_w^T\hat{\mathrm{v}}))}
\end{eqnarray}

同样，我们可以得出(一对的)object function [\ref{ofnscbow}]，梯度 [\ref{ofnscbowgrad1}], [\ref{ofnscbowgrad2}] 和 [\ref{ofnscbowgrad3}]。

\begin{eqnarray}
\label{ofnscbow}
J &=& -\log(\sigma(\mathrm{u}_c^T\hat{\mathrm{v}})) - \sum_{k=1}^K{\log(\sigma(-\bar{\mathrm{u}}_k^T\hat{\mathrm{v}}))}\\
\label{ofnscbowgrad1}
\frac{\partial J}{\partial \mathrm{v}_{c+j}} &=& \frac{1}{2m}\Big\{-\big(1-\sigma(\mathrm{u}_{c}^T\hat{\mathrm{v}})\big)\mathrm{u}_{c}^T + \sum_{k=1}^K{\big(1-\sigma(-\bar{\mathrm{u}}_k^T\hat{\mathrm{v}})\big)\bar{\mathrm{u}}_k^T}\Big\} \\
\label{ofnscbowgrad2}
\frac{\partial J}{\partial \mathrm{u}_{c}} &=& -\big(1-\sigma(\mathrm{u}_{c}^T\hat{\mathrm{v}})\big)\hat{\mathrm{v}}^T \\
\label{ofnscbowgrad3}
\frac{\partial J}{\partial \bar{\mathrm{u}}_k} &=& \big(1-\sigma(-\bar{\mathrm{u}}_k^T\hat{\mathrm{v}})\big)\hat{\mathrm{v}}^T
\end{eqnarray}

\subsection{GloVe}

我们考虑结合Statistical Based Method和Iteration Based Method的优点——GloVe，其object function为：

\begin{eqnarray}
J = \frac{1}{2}\sum_{i,j=1}^{\lvert V\rvert}{f(P_{i,j})\big(\mathrm{u}_i^T\mathrm{v}_j-\log P_{i,j}\big)^2}
\end{eqnarray}

其中$f(x)=\max(1,2x)$，$\mathrm{u}_i,\mathrm{v}_j$为输出输入向量，$P_{i,j}$为单词$i,j$联合在窗口中出现的频率。

\subsection{Tips}

我们考虑一些实现的技巧，具体如下：

\begin{itemize}
	\item 最后的词向量是什么？可以简单地让$\text{word vector} = \text{input vector} + \text{output vector}$。
	\item 超参数的选择：$n \sim [25,1000]$, $m \sim ?$。
	\item 参数的初始化：用小的随机数初始化。
\end{itemize}

\subsection{Evaluation: Intrinsic and Extrinsic}

我们考虑对word vectord的评估，在实际的应用中，word vector的模型是整个机器学习系统中的某一个子系统，并且这样的一个模型受到其超参数(hyperparameter)的影响，我们需要调整hyperparameter参数使得整个系统的表现尽量得好。

首先考虑Intrinsic Evaluation，因为调整超参数后运行整个机器学习的系统会非常耗时，所以我们仅仅考虑word vector这样一个子系统，我们需要人为定义一个指标$P$来衡量word vector这个子系统的表现的好坏，同时，我们希望这样一个人为的指标$P$与整个机器系统的表现水平$P_f$正相关。

然后我们考虑Extrinsic Evaluation，Extrinsic Evaluation就是考虑整个系统表现水平。

\subsubsection*{Intrinsic Evaluation: Word Vector Analogies}

我们考虑word vector在word vector analogies这项指标上的好坏，考虑这样一组不完整的word analogy:

$$a:b::c:\text{?}$$

我们通过训练好的word vector求出对应$?$的$d$[\ref{wad}]，然后再看是否和实际情况匹配。

\begin{eqnarray}
\label{wad}
d=\argmax_{i}{\frac{(\mathrm{x}_b-\mathrm{x}_a+\mathrm{x}_c)^T\mathrm{x}_i}{\|\mathrm{x}_b-\mathrm{x}_a+\mathrm{x}_c \|}}
\end{eqnarray}

这样的word vector analogies主要有两种，syntax的和semantic的，其要求形式上就是$\mathrm{x}_b-\mathrm{x}_a \approx \mathrm{x}_d-\mathrm{x}_c$，下面是一些例子：

\begin{itemize}
	\item semantic: queen:king::actress:actor, Chicago:Illinois::Austin:Texas
	\item syntax: bad:worst::big:biggest, dancing:danced::flying:fly
\end{itemize}

在具体评测中我们考虑以下几个超参数：

\begin{itemize}
	\item 词向量的维数$n$。
	\item corpus的来源和数量
	\item context window size $m$
	\item context的对称性
\end{itemize}

我们评估word vector在word vector analogies上的表现(准确率)，我们可以得出以下的一些结论：

\begin{itemize}
	\item[1.] 目前最好的是GloVe，准确率最高在$75.0\%$(整体), $81.9\%$(semantics), $69.3\%$(syntax)。
	\item[2.] 一般而言，数据集越大，效果越好；wiki的数据相比较于报刊的数据效果更好。
	\item[3.] $n$的取值比较适中的时候效果最好。
	\item[4.] $m$越大，semantic的效果越好，syntax的效果先升后降。
\end{itemize}

\subsubsection*{Intrinsic Evaluation: Correlation Evaluation}

另外一种评价方法是给定一对单词，让计算机和人独立评估这两个单词的相关性$[0,10]$，比较两者的结果。

\subsubsection*{Extension: $\text{Ambiguity}^*$}

Improving word representations via global context and multiple word prototypes (Huang et al, 2012)

主要思想为将k-means和iteration methods相结合。

\subsubsection*{Extrinsic Tasks}

我们考虑一些简单的NLP的任务：

\begin{itemize}
	\item named-entity recognition: 给一些专有名词分种类(比如John是人，2006是时间)。
	\item sentiment analysis: 判断一个单词／短语／句子的情感是积极的还是消极的。
\end{itemize}

实际上，将这些问题形式化了之后都是解决这样的分类问题：

给定数据集

$$\{x^{(i)},y^{(i)}\}_1^N$$

其中$x^{(i)}$为原始文本，$y^{(i)}$为给此文本做的分类标注(假设是one-hot vector，并且$\in \mathbb{R}^C$)，那么我们就可以用最基本的softmax线性分类器来进行训练。

在已经获得了word vector的前提下，我们就可以用word vector代替原文本作为分类器的输入，在具体实现上可以把一个单词作为输入，也可以把几个单词(一个window)作为输入(window classification)。同时，我们不光可以训练分类器的参数$W$，也可以再训练word vector $\mathrm{x}$，如果要进行对word vector的再训练的话，我们需要保证训练的数据集足够大使得其几乎覆盖word vector中的全部的单词。


\section{Neural Networks}

\subsection{Max-Margin Object Function}

我们考虑一个二元分类问题中一个与传统的交叉熵形式不同的object function，max-margin object function，我们会最小化

\begin{eqnarray}
\label{obmm}
J=\max(\Delta+s_2-s_1, 0)
\end{eqnarray}

在这里面，我们假设对于特征空间中的每个点，我们通过神经网络(计算图)都能求出一个score $s$，我们不妨假设我们可以做到使得第一类中score的最大值$s_1$小于第二类中score的最小值$s_2$，我们希望两个类在(通过神经网络)投影到$\mathbb{R}$上时存在一个长度为$\Delta$的边界，即$s_1+\Delta \le s_2$，而作为object function的式[\ref{obmm}]就能够实现这一点。

\subsection{Neuron, Terminology, Foward Propagation}

我们考虑一个神经元的组成，一个神经元上(结点标号为$i$)的Foward Propagation通常由以下两步组成：

\begin{eqnarray}
z_i &=& b_i + \sum_{j}{a_j w_{ij}} \\
a_i &=& f(z_i)
\end{eqnarray}

其中$w_{ij}$为weights，$b_i$为bias，都是需要学习的参数；$f(\cdot)$为activation function。对于整个网络的Foward Propagation，我们先对网络进行拓扑排序，然后按拓扑序对每个节点分别做Propagation。

我们一般会使用fully-connected的神经网络，通常这种网络会有layer的概念，layer分为input layer $\mathrm{x}$，hidden layer $\mathrm{h}^{(i)}$，output layer $\mathrm{y}$，一般而言最后的输出$y=g(\mathrm{s})$, 这里的$\mathrm{s}$称为score，在这样分层数的神经网络中的Forward Propagation过程如下：

\begin{eqnarray}
\mathrm{h}^{(1)} &=& f(\mathrm{z}^{(1)}) = f(W^{(1)}\mathrm{x}+\mathrm{b}^{(1)}) \\
\mathrm{h}^{(i)} &=& f(\mathrm{z}^{(i)}) = f(W^{(i)}\mathrm{h}^{(i-1)}+\mathrm{b}^{(i)}) \\
\mathrm{y} &=& g(\mathrm{s}) = f(W^{(n+1)}\mathrm{h}^{(n)}+\mathrm{b}^{(n+1)})
\end{eqnarray}

这样的网络通常被称为$n$层的神经网络，一般而言这里的$n$层指的是hidden layer有$n$层，也有的地方说$n+2$层(连带输入输出层)。

上面的三个式子是向量化的表述，在程序实现的时候我们倾向于用向量式的描述实现，其中$\mathrm{z}^{(i)},\mathrm{h}^{(i)},\mathrm{b}^{(i)}\in \mathbb{R}^{m_i}$，$W^{(i)} \in \mathbb{M}_{m_i\times m_{i-1}}$，其中$m_i$为第$i$层的神经元数。

\subsection{Back Propagation, Computational Graph}

我们首先考虑一般神经网络(计算图)的梯度的计算，其核心在于计算[\ref{deltadef}]，通过链式法则我们可以求得其表达式[\ref{deltaeq}]

\begin{eqnarray}
\label{deltadef}
\delta_i &\triangleq& \frac{\partial J}{\partial z_i} \\
\label{deltaeq}
&=& f'(z_i)\sum_{k}{\delta_k w_{ki}}
\end{eqnarray}

那么，相关的参数的偏导也很好求了

\begin{eqnarray}
\frac{\partial J}{\partial w_{ij}} &=& \delta_i a_j \\
\frac{\partial J}{\partial b_{i}} &=& \delta_i
\end{eqnarray}

继续考虑$n$层神经网络的梯度的计算：

\begin{eqnarray}
\delta^{(k)}&=&f'(\mathrm{z}^{(k)})\circ\big({W^{(k)}}^T\delta^{(k+1)}\big)\\
\nabla_{W^{(k)}}&=&\delta^{(k+1)}{\mathrm{a}^{(k)}}^T
\end{eqnarray}

其中$\circ$为向量的element-wise的乘法运算。

\subsection{Tips and Tricks}

整个神经网络的设计调试包括以下几个方面：

\begin{itemize}
	\item[1.] 设计神经网络的结构，包括输入输出层，中间的连接层(RNN, CNN, FULL)。
	\item[2.] 对于全连接层，设计activation function。
	\item[3.] 调试，主要进行gradient check。
	\item[4.] 参数初始化。
	\item[5.] 进行优化过程。
	\item[6.] 判断模型是否足够强大会导致过拟合，如果会导致过拟合的话使用regularization，否则重新设计模型。
\end{itemize}

我们下面依次考虑这些问题。

\subsubsection*{Non-linearities}

主要有以下几种activation function：

\begin{itemize}
	\item $\sigma(z)$: 我们之间介绍的sigmoid函数。
	\item $\tanh(z)$: 一般而言，其效果比sigmoid函数好。
	\item ReLU: $f(z)=max(z, 0)$: 导数计算方便并且不会有gradient vanishing的问题。
\end{itemize}

还有一些函数比如hard tanh, soft sign, Leaky ReLU之类。

\subsubsection*{Gradient Check}

通常gradient check，我们使用以下式子

\begin{eqnarray}
f'_i(\theta)\approx\frac{f(\theta^{(i+)})-f(\theta^{(i-)})}{2\epsilon}
\end{eqnarray}

其中$\theta^{(i+)}$为让$\theta$的第$i$维加上$\epsilon$，其余维不变的向量，$\theta^{(i-)}$的含义类似，值得注意的是，在具体实现的时候，每次给第$i$维加上一个$\epsilon$到迭代到第$i+1$维的时候需要恢复第$i$维的结果。

\subsubsection*{Parameter Initialization}

(Xavuer et al. 2010)表示对于$\tanh(z)$来说最好的$W^{(i)}$的初始化的方法是：

\begin{eqnarray}
W \sim \mathcal{U}\Big[ -\sqrt{\frac{6}{m_{i}+m_{i-1}}}, \sqrt{\frac{6}{m_{i}+m_{i-1}}}\Big]
\end{eqnarray}

对于sigmoid函数来说，最好使用$4W$。

\subsubsection*{Optimization Trick}

我们一般使用SGD或者Mini-batch SGD来更新，关于learning rate与更新法则的相关优化有以下几种：

\noindent \textbf{1. Momentum}

改变更新法则变成：

\begin{eqnarray}
v &=& \mu v - \alpha\nabla_\theta J(\theta) \\
\theta^{new} &=& \theta^{old} + v
\end{eqnarray}

一般取$v=0$，$\mu=0.9$。实际上这种方法有一定的物理意义，在于让学习下降的时候不要冲得“太过”。

\noindent \textbf{Adagrad}

在这里面，我们对于每一个参数都设置不同的参数，定义$g_{t,i}=\frac{\partial}{\partial \theta_i^t}J_t(\theta)$，那么，更新法则为：

$$\theta_{t,i}=\theta_{t-1,i}-\frac{\alpha}{\sqrt{\sum_{\tau=1}^{t}{g_{\tau,i}}}}g_{t,i}$$

\subsubsection*{Regularization}

一般而言正则化的方法有以下几个：

\begin{itemize}
	\item 在object function中加入正则项(我们只惩罚$W$不惩罚$b$)。
	\item Early-stopping
	\item Sparsity Contraints，让大多数情况下神经元不被激励。
	\item Dropout：在输入的时候让每一层随机选一半的神经元以0为输入，训练完成后把weights除以2，这样能够有效阻止feature co-adaptation，使得神经网络“记住“了某种模式从而丧失了推广性。
\end{itemize}
\section{Recurrent Neural Networks}

\section{Recursive Neural Networks}

\section{}
\end{document}